{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9d1cdd-048e-400f-bb7d-61f9ee20e0b2",
   "metadata": {},
   "source": [
    "# CourseWork Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ea534-f317-422c-8056-c0127c19f8e0",
   "metadata": {},
   "source": [
    "## California Housing Prices Dataset Preprocessing\n",
    "In this notebook, we will preprocess the California Housing Prices dataset to prepare it for machine learning model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7feff-f158-4f39-96bf-e911544d74f4",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8c627157-7c8d-4214-9f83-80c148c17870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364495a9-9a29-4b89-829d-8ab47c384cf0",
   "metadata": {},
   "source": [
    "The California Housing Dataset was loaded from a publicly available source on Kaggle. In the code above we are loading the dataset ready to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505cb02-c469-44c0-afca-eed9db543307",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "Conducting EDA helps in understanding the structure and contents of the dataset, identifying potential issues such as missing values or outliers, and providing insights into the relationship between features.\n",
    "\n",
    "### Initial Inspection\n",
    "An initial inspection was done to understand the data types, check for missing values, and assess the basic statistics of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d110d77d-7dd2-4fc0-9e05-725f8ad8863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  int64  \n",
      " 3   total_rooms         20640 non-null  int64  \n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  int64  \n",
      " 6   households          20640 non-null  int64  \n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  int64  \n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "Describe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20433.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.569704</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>2635.763081</td>\n",
       "      <td>537.870553</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>499.539680</td>\n",
       "      <td>3.870671</td>\n",
       "      <td>206855.816909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.003532</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2181.615252</td>\n",
       "      <td>421.385070</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>382.329753</td>\n",
       "      <td>1.899822</td>\n",
       "      <td>115395.615874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.800000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1447.750000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.563400</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>179700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3148.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>4.743250</td>\n",
       "      <td>264725.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
       "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
       "std        2.003532      2.135952           12.585558   2181.615252   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
       "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
       "mean       537.870553   1425.476744    499.539680       3.870671   \n",
       "std        421.385070   1132.462122    382.329753       1.899822   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    787.000000    280.000000       2.563400   \n",
       "50%        435.000000   1166.000000    409.000000       3.534800   \n",
       "75%        647.000000   1725.000000    605.000000       4.743250   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  \n",
       "count        20640.000000  \n",
       "mean        206855.816909  \n",
       "std         115395.615874  \n",
       "min          14999.000000  \n",
       "25%         119600.000000  \n",
       "50%         179700.000000  \n",
       "75%         264725.000000  \n",
       "max         500001.000000  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"Head\")\n",
    "data.head()\n",
    "\n",
    "# Get a summary of the dataset, including data types and missing values\n",
    "print(\"Info\")\n",
    "data.info()\n",
    "\n",
    "# Statistical summary of the dataset\n",
    "print(\"Describe\")\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da35e9-b495-48f8-9ec8-b8cac358472f",
   "metadata": {},
   "source": [
    "From the summary we can see some basic information we can interpret:\n",
    "\n",
    "The dataset consists of 20640 rows and 10 columns (features, including continuous variables like longitude, latitude, median_income, and the target variable median_house_value.\n",
    "\n",
    "Missing values were detected in the total_bedrooms column.\n",
    "\n",
    "Continuous variables such as total_rooms and population showed a wide range, indicating potential outliers that could affect model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea2ce0-1864-41ba-9fe4-a9e2f84a53ab",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values\n",
    "\n",
    "The feature total_bedrooms contained missing values. To maintain the integrity of the dataset, these missing values were imputed using the median value of the feature. Median imputation was chosen to reduce the impact of outliers on the imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8fb860d2-a138-4abb-a79c-4e200588e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'total_bedrooms' with the median value\n",
    "data['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].median())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d766f73-4192-49a8-8bf8-170fa40599ac",
   "metadata": {},
   "source": [
    "## 4. Handling Categorical Variables\n",
    "\n",
    "The dataset contains one categorical variable, ocean_proximity, which describes the proximity of the neighborhood to the ocean. Since most machine learning models work with numerical data, this categorical feature was transformed into numeric form using one-hot encoding. One-hot encoding creates new binary columns for each unique category in the ocean_proximity feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5dd89f9b-6eac-429b-8aaf-b9121c5bf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'ocean_proximity' column\n",
    "data = pd.get_dummies(data, columns=['ocean_proximity'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb6a2e-e0fb-468f-9354-7f488d6327f0",
   "metadata": {},
   "source": [
    "## 5. Handling Outliers\n",
    "\n",
    "Outliers in the dataset can distort model performance, especially in regression tasks. Visual inspection through boxplots and histograms helped identify potential outliers in continuous features like total_rooms, population, and households.\r\n",
    "\r\n",
    "Outliers can distort model performance, especially in regression tasks and models like SVM and Neural Networks. We will detect and remove outliers using th**Interquartile Range (IQ** method for continuous features like `total_rooms` and `population`. Any values outside 1.5 times the IQR from the first and third quartiles will be considered outliers and removed.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3d221ce1-0c53-4779-92a5-f6bc45a4ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting and removing outliers using IQR for 'total_rooms'\n",
    "Q1 = data['total_rooms'].quantile(0.25)\n",
    "Q3 = data['total_rooms'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers for 'total_rooms'\n",
    "data = data[(data['total_rooms'] >= lower_bound) & (data['total_rooms'] <= upper_bound)]\n",
    "\n",
    "# Detecting and removing outliers using IQR for 'population'\n",
    "Q1 = data['population'].quantile(0.25)\n",
    "Q3 = data['population'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers for 'population'\n",
    "data = data[(data['population'] >= lower_bound) & (data['population'] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f3701-35ea-40b0-894c-e29380693ef2",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "To enrich the dataset, several new features were engineered from existing ones. These derived features help models capture additional relationships within the data:\n",
    "\n",
    "- `rooms_per_household`: The average number of rooms per household.\n",
    "- `bedrooms_per_room`: The ratio of bedrooms to rooms.\n",
    "- `population_per_household`: The average number of people per household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6bcbbbb0-ef3c-4c2d-bee5-4347c7741f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "data['population_per_household'] = data['population'] / data['households']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98141e61-2175-4c4d-866d-a3bb4a3366bf",
   "metadata": {},
   "source": [
    "## 7. Normalization and Standardization\n",
    "Many machine learning algorithms, such as SVM and Neural Networks, perform better when features are normalized or standardized. Normalization ensures that all features are on the same scale, preventing features with large ranges (like population) from dominating the models. Continuous variables like median_income, housing_median_age, and population were standardized to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "39be48a9-6f89-4a53-8ce4-ce0a4a675fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the continuous variables for scaling\n",
    "continuous_features = ['median_income', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'rooms_per_household', 'bedrooms_per_room', 'population_per_household']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling\n",
    "data[continuous_features] = scaler.fit_transform(data[continuous_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf98642-80c9-4d13-b985-a10d08dd84e4",
   "metadata": {},
   "source": [
    "## 8. Creating the Classification Target\n",
    "In this section, we will create a new target variable for the classification task. The median_house_value column, which represents the median house price, will be converted into a binary classification target. We'll classify houses as \"affordable\" (0) or \"expensive\" (1), using a threshold of $300,000.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e15c48d8-4275-4f04-8b7d-33e704604a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_category\n",
       "0    15251\n",
       "1     3498\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Creating the classification target: 1 for \"expensive\", 0 for \"affordable\"\n",
    "data['price_category'] = np.where(data['median_house_value'] > 300000, 1, 0)\n",
    "\n",
    "# Check the distribution of the new target variable\n",
    "data['price_category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd59b0-e262-4128-86d5-e37f530f05b0",
   "metadata": {},
   "source": [
    "## 9. Train-Test Split\n",
    "Before applying machine learning models, the dataset needs to be split into training and testing sets. We'll use 70% of the data for training and 30% for testing, ensuring that the model is evaluated on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "30abedf7-035e-4e4b-b1b7-4b578f922d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13124, 15), (5625, 15), (13124, 15), (5625, 15))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y) for regression and classification\n",
    "X = data.drop(columns=['median_house_value', 'price_category'])\n",
    "y_regression = data['median_house_value']\n",
    "y_classification = data['price_category']\n",
    "\n",
    "# Split the data for regression\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for classification\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_classification, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check the shape of the training and testing sets\n",
    "X_train_reg.shape, X_test_reg.shape, X_train_clf.shape, X_test_clf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625f320-5aa7-4cef-aacf-35f7b409dd49",
   "metadata": {},
   "source": [
    "## Final Dataset Preparation\n",
    "After preprocessing, the dataset is ready for use in the machine learning models. The dataset now consists of both transformed and engineered features, with missing values handled and all continuous variables standardized. The preprocessing ensures that the dataset is clean, structured, and suitable for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "afd9f919-e2d5-4220-8798-09767e917e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_room</th>\n",
       "      <th>population_per_household</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>0.917835</td>\n",
       "      <td>-1.163092</td>\n",
       "      <td>-1.371840</td>\n",
       "      <td>-1.466437</td>\n",
       "      <td>-1.361638</td>\n",
       "      <td>2.364329</td>\n",
       "      <td>452600</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.644190</td>\n",
       "      <td>-1.034927</td>\n",
       "      <td>-0.346800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.634163</td>\n",
       "      <td>-1.108125</td>\n",
       "      <td>-1.166312</td>\n",
       "      <td>-1.123039</td>\n",
       "      <td>1.801308</td>\n",
       "      <td>352100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.176295</td>\n",
       "      <td>-1.297588</td>\n",
       "      <td>-0.111964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.808070</td>\n",
       "      <td>-0.913581</td>\n",
       "      <td>-1.059371</td>\n",
       "      <td>-0.926545</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>341300</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.168084</td>\n",
       "      <td>-0.452417</td>\n",
       "      <td>-0.354044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.489992</td>\n",
       "      <td>-0.719037</td>\n",
       "      <td>-1.047297</td>\n",
       "      <td>-0.739408</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>342200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.357625</td>\n",
       "      <td>-0.642588</td>\n",
       "      <td>-0.702891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-1.127950</td>\n",
       "      <td>-1.008691</td>\n",
       "      <td>-1.309475</td>\n",
       "      <td>-1.048184</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>269700</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.262696</td>\n",
       "      <td>0.275427</td>\n",
       "      <td>-0.742462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88            0.917835    -1.163092       -1.371840   \n",
       "2    -122.24     37.85            1.808854    -0.634163       -1.108125   \n",
       "3    -122.25     37.85            1.808854    -0.808070       -0.913581   \n",
       "4    -122.25     37.85            1.808854    -0.489992       -0.719037   \n",
       "5    -122.25     37.85            1.808854    -1.127950       -1.008691   \n",
       "\n",
       "   population  households  median_income  median_house_value  \\\n",
       "0   -1.466437   -1.361638       2.364329              452600   \n",
       "2   -1.166312   -1.123039       1.801308              352100   \n",
       "3   -1.059371   -0.926545       0.950132              341300   \n",
       "4   -1.047297   -0.739408       0.002676              342200   \n",
       "5   -1.309475   -1.048184       0.103174              269700   \n",
       "\n",
       "   ocean_proximity_INLAND  ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "0                   False                   False                      True   \n",
       "2                   False                   False                      True   \n",
       "3                   False                   False                      True   \n",
       "4                   False                   False                      True   \n",
       "5                   False                   False                      True   \n",
       "\n",
       "   ocean_proximity_NEAR OCEAN  rooms_per_household  bedrooms_per_room  \\\n",
       "0                       False             0.644190          -1.034927   \n",
       "2                       False             1.176295          -1.297588   \n",
       "3                       False             0.168084          -0.452417   \n",
       "4                       False             0.357625          -0.642588   \n",
       "5                       False            -0.262696           0.275427   \n",
       "\n",
       "   population_per_household  price_category  \n",
       "0                 -0.346800               1  \n",
       "2                 -0.111964               1  \n",
       "3                 -0.354044               1  \n",
       "4                 -0.702891               1  \n",
       "5                 -0.742462               0  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final processed data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc540b33-eec6-4b09-8f94-765f01b0dbf8",
   "metadata": {},
   "source": [
    "## Summary of the Preprocessing\n",
    "The preprocessing steps followed for the California Housing Dataset included loading the dataset, conducting exploratory data analysis, handling missing values, transforming categorical variables, creating new features, and standardizing the continuous variables. These steps ensure that the dataset is ready for machine learning model development, providing a solid foundation for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa09546-b966-4c78-9fac-c4ef9922d5c2",
   "metadata": {},
   "source": [
    "## The Code in One Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9450848c-fa27-4006-b317-fb0b042f6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  int64  \n",
      " 3   total_rooms         20640 non-null  int64  \n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  int64  \n",
      " 6   households          20640 non-null  int64  \n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  int64  \n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "Describe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity_INLAND</th>\n",
       "      <th>ocean_proximity_ISLAND</th>\n",
       "      <th>ocean_proximity_NEAR BAY</th>\n",
       "      <th>ocean_proximity_NEAR OCEAN</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_room</th>\n",
       "      <th>population_per_household</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>0.917835</td>\n",
       "      <td>-1.163092</td>\n",
       "      <td>-1.371840</td>\n",
       "      <td>-1.466437</td>\n",
       "      <td>-1.361638</td>\n",
       "      <td>2.364329</td>\n",
       "      <td>452600</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.644190</td>\n",
       "      <td>-1.034927</td>\n",
       "      <td>-0.346800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.634163</td>\n",
       "      <td>-1.108125</td>\n",
       "      <td>-1.166312</td>\n",
       "      <td>-1.123039</td>\n",
       "      <td>1.801308</td>\n",
       "      <td>352100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.176295</td>\n",
       "      <td>-1.297588</td>\n",
       "      <td>-0.111964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.808070</td>\n",
       "      <td>-0.913581</td>\n",
       "      <td>-1.059371</td>\n",
       "      <td>-0.926545</td>\n",
       "      <td>0.950132</td>\n",
       "      <td>341300</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.168084</td>\n",
       "      <td>-0.452417</td>\n",
       "      <td>-0.354044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-0.489992</td>\n",
       "      <td>-0.719037</td>\n",
       "      <td>-1.047297</td>\n",
       "      <td>-0.739408</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>342200</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.357625</td>\n",
       "      <td>-0.642588</td>\n",
       "      <td>-0.702891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>1.808854</td>\n",
       "      <td>-1.127950</td>\n",
       "      <td>-1.008691</td>\n",
       "      <td>-1.309475</td>\n",
       "      <td>-1.048184</td>\n",
       "      <td>0.103174</td>\n",
       "      <td>269700</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.262696</td>\n",
       "      <td>0.275427</td>\n",
       "      <td>-0.742462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88            0.917835    -1.163092       -1.371840   \n",
       "2    -122.24     37.85            1.808854    -0.634163       -1.108125   \n",
       "3    -122.25     37.85            1.808854    -0.808070       -0.913581   \n",
       "4    -122.25     37.85            1.808854    -0.489992       -0.719037   \n",
       "5    -122.25     37.85            1.808854    -1.127950       -1.008691   \n",
       "\n",
       "   population  households  median_income  median_house_value  \\\n",
       "0   -1.466437   -1.361638       2.364329              452600   \n",
       "2   -1.166312   -1.123039       1.801308              352100   \n",
       "3   -1.059371   -0.926545       0.950132              341300   \n",
       "4   -1.047297   -0.739408       0.002676              342200   \n",
       "5   -1.309475   -1.048184       0.103174              269700   \n",
       "\n",
       "   ocean_proximity_INLAND  ocean_proximity_ISLAND  ocean_proximity_NEAR BAY  \\\n",
       "0                   False                   False                      True   \n",
       "2                   False                   False                      True   \n",
       "3                   False                   False                      True   \n",
       "4                   False                   False                      True   \n",
       "5                   False                   False                      True   \n",
       "\n",
       "   ocean_proximity_NEAR OCEAN  rooms_per_household  bedrooms_per_room  \\\n",
       "0                       False             0.644190          -1.034927   \n",
       "2                       False             1.176295          -1.297588   \n",
       "3                       False             0.168084          -0.452417   \n",
       "4                       False             0.357625          -0.642588   \n",
       "5                       False            -0.262696           0.275427   \n",
       "\n",
       "   population_per_household  price_category  \n",
       "0                 -0.346800               1  \n",
       "2                 -0.111964               1  \n",
       "3                 -0.354044               1  \n",
       "4                 -0.702891               1  \n",
       "5                 -0.742462               0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('housing.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Head\")\n",
    "data.head()\n",
    "\n",
    "# Get a summary of the dataset, including data types and missing values\n",
    "print(\"Info\")\n",
    "data.info()\n",
    "\n",
    "# Statistical summary of the dataset\n",
    "print(\"Describe\")\n",
    "data.describe()\n",
    "\n",
    "# Fill missing values in 'total_bedrooms' with the median value\n",
    "data['total_bedrooms'] = data['total_bedrooms'].fillna(data['total_bedrooms'].median())\n",
    "\n",
    "# One-hot encode the 'ocean_proximity' column\n",
    "data = pd.get_dummies(data, columns=['ocean_proximity'], drop_first=True)\n",
    "\n",
    "# Detecting and removing outliers using IQR for 'total_rooms'\n",
    "Q1 = data['total_rooms'].quantile(0.25)\n",
    "Q3 = data['total_rooms'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers for 'total_rooms'\n",
    "data = data[(data['total_rooms'] >= lower_bound) & (data['total_rooms'] <= upper_bound)]\n",
    "\n",
    "# Detecting and removing outliers using IQR for 'population'\n",
    "Q1 = data['population'].quantile(0.25)\n",
    "Q3 = data['population'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers for 'population'\n",
    "data = data[(data['population'] >= lower_bound) & (data['population'] <= upper_bound)]\n",
    "\n",
    "# Create new features\n",
    "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "data['population_per_household'] = data['population'] / data['households']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the continuous variables for scaling\n",
    "continuous_features = ['median_income', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'rooms_per_household', 'bedrooms_per_room', 'population_per_household']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling\n",
    "data[continuous_features] = scaler.fit_transform(data[continuous_features])\n",
    "\n",
    "# Creating the classification target: 1 for \"expensive\", 0 for \"affordable\"\n",
    "data['price_category'] = np.where(data['median_house_value'] > 300000, 1, 0)\n",
    "\n",
    "# Check the distribution of the new target variable\n",
    "data['price_category'].value_counts()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y) for regression and classification\n",
    "X = data.drop(columns=['median_house_value', 'price_category'])\n",
    "y_regression = data['median_house_value']\n",
    "y_classification = data['price_category']\n",
    "\n",
    "# Split the data for regression\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the data for classification\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X, y_classification, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check the shape of the training and testing sets\n",
    "X_train_reg.shape, X_test_reg.shape, X_train_clf.shape, X_test_clf.shape\n",
    "\n",
    "\n",
    "# Check the final processed data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f6a71-6777-4054-8119-33d84707b3ac",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "In this section, we implement and evaluate three machine learning algorithms—Decision Trees, Support Vector Machines (SVM), and Neural Networks (Multi-Layer Perceptron, MLP)—on the preprocessed dataset. The objective is to predict whether a neighborhood is classified as \"expensive\" (house prices above $300,000) or \"affordable\" (house prices below or equal to $300,000) based on various features. We will also perform hyperparameter tuning for each model to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64368359-2e4e-43e8-8905-c704afcc9595",
   "metadata": {},
   "source": [
    "## 1. Machine Learning Algorithms\n",
    "We selected three distinct machine learning algorithms that represent different paradigms of machine learning. Each algorithm brings unique strengths, and their performance on this dataset will provide insights into the suitability of each approach for the classification task.\n",
    "\n",
    "### 1.1 Decision Trees\n",
    "Decision Trees classify data by recursively splitting the dataset based on feature values, forming a tree structure where each leaf represents a classification decision. The algorithm is easy to interpret, making it an ideal choice for understanding the relationships between the features and the target variable. However, decision trees are prone to overfitting, which is why hyperparameter tuning is crucial.\n",
    "\n",
    "### What We Are Testing for in the Decision Tree Model\n",
    "Target Variable: The binary classification target, price_category, which classifies neighborhoods as \"expensive\" (1) or \"affordable\" (0).\n",
    "\n",
    "Features: All features, including longitude, latitude, median_income, total_rooms, and engineered features like rooms_per_household and bedrooms_per_room.\n",
    "\n",
    "### Hyperparameters Tuned:\n",
    "max_depth: Controls how deep the tree can grow. A deeper tree can model complex relationships but may overfit the data.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Higher values prevent overfitting by limiting tree growth.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node, controlling the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cddec7ac-c869-4e81-936d-4df7bc43e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Decision Tree: {'max_depth': None, 'min_samples_leaf': 10, 'min_samples_split': 20}\n",
      "Decision Tree Accuracy: 0.9033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid_dt = {'max_depth': [3, 5, 10, None],\n",
    "                 'min_samples_split': [2, 10, 20],\n",
    "                 'min_samples_leaf': [1, 5, 10]}\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation\n",
    "grid_search_dt = GridSearchCV(decision_tree, param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_dt.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Retrieve and print the best hyperparameters\n",
    "best_dt_params = grid_search_dt.best_params_\n",
    "print(f\"Best Hyperparameters for Decision Tree: {best_dt_params}\")\n",
    "\n",
    "# Train the best model with the optimal hyperparameters\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "best_dt_model.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Evaluate the trained model on the test data\n",
    "dt_accuracy = best_dt_model.score(X_test_clf, y_test_clf)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c9326-8166-42e6-9325-c5a868cc04e6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "The decision tree model was tuned using a GridSearchCV approach to find the optimal combination of hyperparameters such as max_depth, min_samples_split, and min_samples_leaf. These parameters control the complexity of the tree and prevent overfitting. A cross-validation procedure (5-fold) was used to ensure that the model generalizes well to unseen data.\n",
    "\n",
    "### Evaluation:\n",
    "The model’s accuracy on the test set is evaluated, and the best-performing decision tree is chosen based on the hyperparameters that maximize the cross-validated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ab390-7148-4a00-89bd-445ed3ee781f",
   "metadata": {},
   "source": [
    "### 1.2 Support Vector Machine (SVM)\n",
    "Support Vector Machines (SVM) classify data by finding the optimal hyperplane that separates the data points of different classes with the maximum margin. SVM is particularly useful when the data is not linearly separable, as it uses kernel functions to map the data into higher-dimensional spaces. The choice of kernel function and other hyperparameters significantly affects SVM’s performance.\n",
    "\n",
    "### What We Are Testing for in the SVM Model\n",
    "Target Variable: The binary classification target, price_category.\n",
    "\n",
    "Features: All features, including both continuous variables like median_income and categorical variables (after encoding) like ocean_proximity.\n",
    "\n",
    "### Hyperparameters Tuned:\n",
    "C: The regularisation parameter that controls the trade-off between maximizing the margin and minimizing classification error.\n",
    "\n",
    "kernel: Specifies the kernel function used to transform the data. Common kernels are linear and RBF (Radial Basis Function).\n",
    "\n",
    "gamma: Determines the influence of a single training example. Higher values lead to tighter decision boundaries, while lower values result in smoother decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b57e8f19-e539-4929-9abb-87333094ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for SVM: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "SVM Accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define a parameter grid for tuning the hyperparameters of the SVM\n",
    "param_grid_svm = {'C': [0.1, 1, 10], \n",
    "                  'kernel': ['linear', 'rbf'], \n",
    "                  'gamma': ['scale', 'auto']}\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Retrieve and print the best hyperparameters\n",
    "best_svm_params = grid_search_svm.best_params_\n",
    "print(f\"Best Hyperparameters for SVM: {best_svm_params}\")\n",
    "\n",
    "# Train the best SVM model with the optimal hyperparameters\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "best_svm_model.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Evaluate the trained model on the test data\n",
    "svm_accuracy = best_svm_model.score(X_test_clf, y_test_clf)\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af80fc9-6add-42c7-9718-9680d4150f0b",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "The SVM model was tuned using the parameters C, kernel, and gamma. The C parameter controls the trade-off between maximizing the margin and minimizing classification error, while the kernel determines the transformation used for mapping the data into higher dimensions. The RBF kernel was tested for non-linear relationships, while the linear kernel was included for simpler cases. Gamma controls the influence of a single training example, determining the shape of the decision boundary.\n",
    "\n",
    "### Evaluation:\n",
    "After hyperparameter tuning, the SVM model is evaluated on the test set, and the results are compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ca8a0-7d64-4963-bd5d-3772d0e52c0a",
   "metadata": {},
   "source": [
    "## 1.3 Neural Network (MLP)\n",
    "Neural Networks (MLP) are capable of learning complex patterns in data by using multiple layers of interconnected neurons. Each layer transforms the data, allowing the model to capture non-linear relationships. Neural Networks are particularly powerful when dealing with complex datasets, but they can be computationally expensive to train. The architecture (number of layers and neurons) and activation function play a key role in their performance.\n",
    "\n",
    "### What We Are Testing for in the Neural Network (MLP) Model\n",
    "\n",
    "Target Variable: The binary classification target, price_category.\n",
    "\n",
    "Features: All features, including engineered features such as rooms_per_household and population_per_household.\n",
    "\n",
    "### Hyperparameters Tuned:\n",
    "hidden_layer_sizes: Controls the number of neurons and layers in the network. A larger network can capture more complex relationships but requires more data to avoid overfitting.\n",
    "\n",
    "activation: The activation function used to determine how neurons fire. Common choices are relu (Rectified Linear Unit) and tanh.\n",
    "\n",
    "alpha: The regularization parameter that helps prevent overfitting by controlling the magnitude of weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "caa9825d-e6fc-4fbc-970b-44eb2137f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Neural Network: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "Neural Network Accuracy: 0.9028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Initialize the Neural Network model\n",
    "mlp_model = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Define a parameter grid for tuning the neural network's hyperparameters\n",
    "param_grid_mlp = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], \n",
    "                  'activation': ['relu', 'tanh'], \n",
    "                  'alpha': [0.0001, 0.001, 0.01]}\n",
    "\n",
    "# Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation\n",
    "grid_search_mlp = GridSearchCV(mlp_model, param_grid_mlp, cv=5, scoring='accuracy')\n",
    "grid_search_mlp.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Retrieve and print the best hyperparameters\n",
    "best_mlp_params = grid_search_mlp.best_params_\n",
    "print(f\"Best Hyperparameters for Neural Network: {best_mlp_params}\")\n",
    "\n",
    "# Train the best MLP model with the optimal hyperparameters\n",
    "best_mlp_model = grid_search_mlp.best_estimator_\n",
    "best_mlp_model.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Evaluate the trained model on the test data\n",
    "mlp_accuracy = best_mlp_model.score(X_test_clf, y_test_clf)\n",
    "print(f\"Neural Network Accuracy: {mlp_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afffdc-9458-47e4-a7b1-78040074b280",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "The Multi-Layer Perceptron (MLP) classifier was tuned for parameters like hidden_layer_sizes (number of neurons and layers), activation functions (e.g., ReLU or tanh), and the regularization parameter alpha. These hyperparameters control the complexity and flexibility of the network. The activation function dictates how neurons fire, while the hidden layers control the network's capacity to learn complex patterns.\n",
    "\n",
    "### Evaluation:\n",
    "The model’s accuracy is evaluated on the test data, and its performance is compared to the SVM and Decision Tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee43df-805c-4363-ba0a-f969efb7596e",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "For each model, hyperparameter tuning was performed using GridSearchCV, which automates the process of testing various hyperparameter combinations to find the optimal configuration. Cross-validation (5-fold) was applied to ensure that the models generalize well to unseen data, reducing the risk of overfitting. The following hyperparameters were tuned for each model:\n",
    "\n",
    "- Decision Tree: max_depth, min_samples_split, min_samples_leaf\n",
    "- SVM: C, kernel, gamma\n",
    "- Neural Networks (MLP): hidden_layer_sizes, activation, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3e408-3da5-4328-b23e-e2d6b0386fa8",
   "metadata": {},
   "source": [
    "## 3. Strengths and Weaknesses of Each Model\n",
    "\n",
    "### 3.1 Decision Trees\n",
    "Strengths:\n",
    "\n",
    "- Interpretability: One of the key advantages of decision trees is their interpretability. They allow you to visualise the decision-making process, making it easy to understand how the model reaches its predictions. This makes decision trees particularly useful when model transparency is important.\n",
    "\n",
    "- Handling Non-Linear Data: Decision trees are capable of handling non-linear data by splitting the feature space recursively. They can model complex decision boundaries without the need for kernel transformations.\n",
    "\n",
    "- No Need for Feature Scaling: Unlike models like SVM or neural networks, decision trees do not require feature scaling, as they rely on the relative ordering of feature values.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Prone to Overfitting: Decision trees tend to overfit the data if they are not pruned or limited in depth. This means that they can perform well on training data but may generalise poorly to new, unseen data.\n",
    "\n",
    "- Instability: Small changes in the dataset can lead to entirely different tree structures, as decision trees are sensitive to the specific splits chosen early in the process.\n",
    "\n",
    "- Limited Expressiveness for Complex Patterns: While decision trees are flexible, they may struggle to model highly complex patterns in the data without becoming overly complex and prone to overfitting.\n",
    "\n",
    "### 3.2 Support Vector Machine (SVM)\n",
    "Strengths:\n",
    "\n",
    "- Effective in High-Dimensional Spaces: SVM is well-suited for classification tasks with high-dimensional feature spaces. It can be particularly powerful when the number of features exceeds the number of samples.\n",
    "Robust to Overfitting with Proper Regularization: By tuning the regularization parameter C, SVM can prevent overfitting, even in complex datasets.\n",
    "\n",
    "- Flexible Kernel Functions: The use of different kernel functions (e.g., linear, RBF) allows SVM to handle both linear and non-linear relationships in the data. The RBF kernel, in particular, is very powerful for capturing non-linear decision boundaries.\n",
    "\n",
    "- Generalization: SVM has strong generalization capabilities, especially when tuned properly. It finds the optimal margin between classes, leading to good performance on unseen data.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Computational Complexity: SVM can be computationally expensive, particularly for large datasets. The time complexity increases significantly with the number of samples, especially when using non-linear kernels.\n",
    "\n",
    "- Sensitive to Feature Scaling: SVM relies heavily on the distances between data points, making it sensitive to feature scaling. Proper normalization or standardization of features is necessary for optimal performance.\n",
    "\n",
    "- Difficult Hyperparameter Tuning: The performance of SVM is highly dependent on the proper selection of hyperparameters (e.g., C, gamma). Without careful tuning, the model may not achieve optimal results.\n",
    "\n",
    "### 3.3 Neural Networks (MLP)\n",
    "Strengths:\n",
    "\n",
    "- Ability to Model Complex Relationships: Neural networks are powerful for capturing complex, non-linear relationships in the data. With enough layers and neurons, they can model intricate patterns that simpler models like decision trees or linear models may miss.\n",
    "\n",
    "- Flexible Architecture: The architecture of a neural network can be adjusted to suit the complexity of the data. The number of hidden layers, neurons, and the choice of activation functions allow the model to be tailored to specific tasks.\n",
    "\n",
    "- Performance with Large Datasets: Neural networks generally perform well with larger datasets, especially when there is enough data to capture and learn the underlying patterns without overfitting.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "- Computationally Intensive: Neural networks require significant computational resources, particularly when the network architecture is complex. Training deep networks can be slow and requires hardware like GPUs for efficiency.\n",
    "\n",
    "- Requires Large Training Data: Neural networks tend to perform poorly with small datasets, as they are prone to overfitting due to their large capacity. They require large amounts of data to generalize effectively.\n",
    "\n",
    "- Black Box Nature: Unlike decision trees, neural networks are often considered \"black boxes.\" Their decision-making process is not easily interpretable, making it difficult to understand why a certain prediction was made.\n",
    "\n",
    "- Sensitive to Hyperparameters: Neural networks require careful tuning of hyperparameters like the learning rate, number of hidden layers, and regularization terms to prevent overfitting and ensure good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699d7a1-b77d-4d29-9c2f-889c9d86d38c",
   "metadata": {},
   "source": [
    "## 4. Reflection on Challenges and Lessons Learned\n",
    "\n",
    "Throughout this project, several challenges were encountered that helped shape the learning experience and the evaluation of the models.\n",
    "\n",
    "### Challenges Faced:\n",
    "1. **Hyperparameter Tuning**: \n",
    "   Tuning the hyperparameters for each model, especially for Support Vector Machines and Neural Networks, was challenging. SVM's performance is highly dependent on selecting the correct `C` and `gamma` values, while Neural Networks require careful tuning of the architecture (e.g., number of layers, neurons, and activation functions) to avoid overfitting or underfitting.\n",
    "\n",
    "2. **Computational Complexity**:\n",
    "   Neural Networks and SVMs, particularly when using non-linear kernels or deeper architectures, required significantly more computational resources. This led to longer training times, which posed a challenge when iterating through different hyperparameter combinations.\n",
    "\n",
    "3. **Dealing with Imbalanced Data**:\n",
    "   Some imbalance in the `price_category` target variable required careful consideration, as models may bias toward the majority class. While we used cross-validation and tuned hyperparameters, this remained a challenge.\n",
    "\n",
    "### Lessons Learned:\n",
    "1. **Model Selection for Complex Tasks**: \n",
    "   SVM demonstrated superior generalization capabilities, making it ideal for this task. The use of the RBF kernel enabled it to model non-linear patterns, which helped it achieve the highest accuracy of the three models. This reflects the strength of SVM in handling complex classification tasks, especially when feature scaling and dimensionality are considerations.\n",
    "\n",
    "2. **Trade-offs in Interpretability**: \n",
    "   Decision Trees provided a level of interpretability that neither SVM nor Neural Networks could match. While its performance was slightly lower, the ease of understanding how the model made predictions reinforced the importance of considering interpretability in model selection, especially in scenarios where understanding decisions is crucial.\n",
    "\n",
    "3. **Neural Networks and Data Requirements**: \n",
    "   Neural Networks, while powerful, require substantial amounts of data to perform well. In this project, MLP performed well but did not outperform SVM, likely due to the size and complexity of the dataset. This emphasized the importance of selecting the right model for the available data and resources.\n",
    "\n",
    "4. **Why SVM Was Best Suited**:\n",
    "   SVM was particularly effective because it balanced accuracy, generalization, and computational feasibility for this classification task. Its ability to handle non-linear decision boundaries without overfitting and its performance in high-dimensional spaces made it the best-performing model. \n",
    "\n",
    "In conclusion, each model had its strengths, but the SVM's robustness and performance made it well-suited for this particular complex classification task.plex classification task.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a0469-6eb5-47db-b462-6fa2c835fd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
